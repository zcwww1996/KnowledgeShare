[TOC]

### 一：浏览器伪装技术原理：

我们尝试着爬取csdn的博客，可以发现返回403的状态码，因为对方服务器会对爬虫进行屏蔽。此时我们需要伪装成浏览器进行爬取。我们一般都过报头进行浏览器的伪装。

### 二：实战

浏览器的网页的 报头中用 User-Agent 字段对应的值来判断是否是浏览器。

所以如果要模拟成浏览器就要在请求的时候对报文进行修改，将User-Agent的值改成对应的浏览器应该有的值。
![](index_files/_u62A5_u5934_u7684_u83B7_u53D6.jpg)


```python
import urllib.request
url="https://blog.csdn.net/weixin_41167340"
# 伪装成浏览器，设置报头信息
# useragent的值
headers=("User-Agent","Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Mobile Safari/537.36")
# 添加报头信息用buid_opener
opener=urllib.request.build_opener()
opener.addheaders=[headers]
#已经伪装称浏览器
data=opener.open(url).read()
fh=open("C:\\Users\\DuJuan\\Desktop\\python\\浏览器伪装\\浏览器伪装.html","wb")
fh.write(data)
fh.close()
```

此时打开本地的网页：出现了爬取的网页。


![](index_files/_u722C_u53D6_u7F51_u9875_u51FA_.jpg)